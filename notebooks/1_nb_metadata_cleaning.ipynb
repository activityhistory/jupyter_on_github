{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "8c58a71854ced"
   },
   "source": [
    "# Notebook Metadata Cleaning\n",
    "\n",
    "This notebook cleans and profiles the metadata that was collected with GitHub's Search AP on the 1.25 million or so Jupyter Notebook hosted publicly on GitHub. This metadata starts as a series of json files detailing the results of each search for notebooks and ends with a cleaned and deduplicated dataframe with the most relevant metadata for each notebook being saved to a csv file for later use.\n",
    "\n",
    "In the end, we have 1,253,620 unique notebooks with data about them saved in `cleaned_nb_data.csv`. We are missing 43,304 notebooks due to the query not being able to return more than 1,000 results per query and having filesizes with more than 1,000 results for the filesize. Most of these are remarkably small files (~35,000 of them are empty 72 byte files). We are additionally missing 2 notebooks that were not returned with the query results. In total, we have notebook metadata on 1,253,620 of 1,296,926 possible notebooks, or 96.66% of all notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "9508a389ffea6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "78a983a483ecd"
   },
   "source": [
    "## Missing Ranges\n",
    "\n",
    "Due to the nature of our search, we may have missed a few files. This would occur when more than 1,000 notebooks had exactly the same filesize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "comet_cell_id": "8cc9f96592e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOO MANY RESULTS: 72-72 bytes, 34895 results\n",
      "\n",
      "TOO MANY RESULTS: 181-181 bytes, 3303 results\n",
      "\n",
      "TOO MANY RESULTS: 301-301 bytes, 1032 results\n",
      "\n",
      "TOO MANY RESULTS: 581-581 bytes, 1416 results\n",
      "\n",
      "TOO MANY RESULTS: 582-582 bytes, 1300 results\n",
      "\n",
      "TOO MANY RESULTS: 1134-1134 bytes, 1358 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open ('../logs/nb_metadata_query_log.txt', 'r') as log:\n",
    "    for l in log:\n",
    "        if l.startswith('TOO MANY RESULTS'):\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "34be6328598bb"
   },
   "source": [
    "This is not too many, just 6 file sizes totaling about 43,000 files. Most of these are remarkably small (72 bytes) with 1134 bytes being the largest. Only this largest set seems to have any content. Everything below it (e.g. 582 bytes and below) are just empty files, or files with one empty cell, so it is not a problem that we missed these. However we should ote that we would have close to 1.3 million files if we kept these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "43bc1e5b5e2c9"
   },
   "source": [
    "## Decide what data to collect\n",
    "\n",
    "First, let's look at what kind of data we have access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "comet_cell_id": "2b1dba1fb8f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['total_count', 'incomplete_results', 'items'])\n",
      "\n",
      "dict_keys(['name', 'path', 'sha', 'url', 'git_url', 'html_url', 'repository', 'score'])\n",
      "\n",
      "dict_keys(['id', 'name', 'full_name', 'owner', 'private', 'html_url', 'description', 'fork', 'url', 'forks_url', 'keys_url', 'collaborators_url', 'teams_url', 'hooks_url', 'issue_events_url', 'events_url', 'assignees_url', 'branches_url', 'tags_url', 'blobs_url', 'git_tags_url', 'git_refs_url', 'trees_url', 'statuses_url', 'languages_url', 'stargazers_url', 'contributors_url', 'subscribers_url', 'subscription_url', 'commits_url', 'git_commits_url', 'comments_url', 'issue_comment_url', 'contents_url', 'compare_url', 'merges_url', 'archive_url', 'downloads_url', 'issues_url', 'pulls_url', 'milestones_url', 'notifications_url', 'labels_url', 'releases_url', 'deployments_url'])\n",
      "\n",
      "dict_keys(['login', 'id', 'avatar_url', 'gravatar_id', 'url', 'html_url', 'followers_url', 'following_url', 'gists_url', 'starred_url', 'subscriptions_url', 'organizations_url', 'repos_url', 'events_url', 'received_events_url', 'type', 'site_admin'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/notebooks/github_notebooks_8948_8988_p1.json') as f:\n",
    "    test_file = json.load(f)\n",
    "    print(test_file.keys())\n",
    "    print(\"\")\n",
    "    print(test_file['items'][0].keys())\n",
    "    print(\"\")\n",
    "    print(test_file['items'][0]['repository'].keys())\n",
    "    print(\"\")\n",
    "    print(test_file['items'][0]['repository']['owner'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "6a004f40d8d28"
   },
   "source": [
    "Based on inspecting an example of the data included with the search results above, let's include the following data in our dataframe.\n",
    "\n",
    "1. html_url\n",
    "2. name\n",
    "3. path\n",
    "4. repository\n",
    "    1. description\n",
    "    2. fork\n",
    "    3. html_url\n",
    "    4. id\n",
    "    5. name\n",
    "    6. owner\n",
    "        1. id\n",
    "        2. html_url\n",
    "        3. login\n",
    "    7. private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "4af47f9c6de77"
   },
   "source": [
    "## Move data from distributed JSON files to a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "comet_cell_id": "28590bf94e3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 13538 data files processed\n",
      "1000 / 13538 data files processed\n",
      "2000 / 13538 data files processed\n",
      "3000 / 13538 data files processed\n",
      "4000 / 13538 data files processed\n",
      "5000 / 13538 data files processed\n",
      "6000 / 13538 data files processed\n",
      "7000 / 13538 data files processed\n",
      "8000 / 13538 data files processed\n",
      "9000 / 13538 data files processed\n",
      "10000 / 13538 data files processed\n",
      "11000 / 13538 data files processed\n",
      "12000 / 13538 data files processed\n",
      "13000 / 13538 data files processed\n",
      "\n",
      "We have 1261705 notebooks\n"
     ]
    }
   ],
   "source": [
    "all_nbs = []\n",
    "\n",
    "# get the names of all our data files\n",
    "nb_search_files = [f for f in os.listdir('data/notebooks') if f[-5:] == '.json']\n",
    "\n",
    "# a subset list that was used for debugging the following code\n",
    "small_list = nb_search_files[0:100]\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = 'nb_metadata_cleaning_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "for j, f in enumerate(nb_search_files):\n",
    "    \n",
    "    # keep track of our progress\n",
    "    if j % 1000 == 0:\n",
    "        print(\"%s / %s data files processed\" % (j, len(nb_search_files)))\n",
    "    \n",
    "    with open('data/notebooks/' + f, 'r') as json_file:\n",
    "        file_components = f.split(\".\")[0].split('_')\n",
    "        \n",
    "        # get data from the filename\n",
    "        min_filesize = file_components[2]\n",
    "        max_filesize = file_components[3]\n",
    "        query_page = file_components[4][1:]\n",
    "        \n",
    "        file_dict = json.load(json_file)\n",
    "        \n",
    "        # do a little tracking of where we may be missing data\n",
    "        if 'incomplete_results' in file_dict:\n",
    "            if file_dict['incomplete_results'] == True:\n",
    "                msg = \"%s has incomplete results\" % f\n",
    "                write_to_log(msg)\n",
    "        \n",
    "        \n",
    "        if 'items' in file_dict:\n",
    "            # track if we have no items in this file\n",
    "            if len(file_dict['items']) == 0:\n",
    "                msg = \"%s has 0 items\" % f\n",
    "                write_to_log(msg)\n",
    "            \n",
    "            # of save all the data for each item\n",
    "            else:\n",
    "                for i in file_dict['items']:\n",
    "                    nb_stats = {}\n",
    "                    nb_stats['html_url'] = i['html_url']\n",
    "                    nb_stats['name'] = i['name']\n",
    "                    nb_stats['path'] = i['path']\n",
    "                    nb_stats['repo_description'] = i['repository']['description']\n",
    "                    nb_stats['repo_fork'] = i['repository']['fork']\n",
    "                    nb_stats['repo_html_url'] = i['repository']['html_url']\n",
    "                    nb_stats['repo_id'] = i['repository']['id']\n",
    "                    nb_stats['repo_name'] = i['repository']['name']\n",
    "                    nb_stats['owner_id'] = i['repository']['owner']['id']\n",
    "                    nb_stats['owner_html_url'] = i['repository']['owner']['html_url']\n",
    "                    nb_stats['owner_login'] = i['repository']['owner']['login']\n",
    "                    nb_stats['repo_private'] = i['repository']['private']\n",
    "                    nb_stats['min_filesize'] = min_filesize\n",
    "                    nb_stats['max_filesize'] = max_filesize\n",
    "                    nb_stats['query_page'] = query_page\n",
    "                    \n",
    "                    all_nbs.append(nb_stats)\n",
    "                    \n",
    "        else:\n",
    "            msg = \"%s has no items object\" % f\n",
    "            write_to_log(msg)    \n",
    "\n",
    "print(\"\")            \n",
    "print(\"We have %s notebooks\"% len(all_nbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "e62d8f1c6aac4"
   },
   "source": [
    "## Check For Missing Notebooks \n",
    "Now that we have the data loaded, we want to check if we have any query result files with missing, or fewer than expected notebooks described in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "f088a319d6159"
   },
   "outputs": [],
   "source": [
    "no_items = []\n",
    "incomplete = []\n",
    "\n",
    "with open('nb_metadata_cleaning_log.txt', 'r') as log:\n",
    "    for l in log:\n",
    "        parts = l.split()\n",
    "        \n",
    "        if parts[2] == '0':\n",
    "            no_items.append(parts[0])\n",
    "        \n",
    "        # if only one incomplete flag, or have more than 100 results, or is the last in the line, don't worry\n",
    "        elif parts[2] == 'incomplete':\n",
    "            with open('data/' + parts[0], 'r') as json_file:                            \n",
    "                file_dict = json.load(json_file)\n",
    "                if len(file_dict['items']) < 100:\n",
    "                    page = int(parts[0].split('_')[-1].split('.')[0][1:])\n",
    "                    if not len(file_dict['items']) + 100 * (page-1) == file_dict['total_count']:\n",
    "                        incomplete.append([parts[0], len(file_dict['items']), file_dict['total_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "comet_cell_id": "ff9eb4b399259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['github_notebooks_3846_3862_p4.json', 98, 688], ['github_notebooks_3846_3862_p4.json', 98, 688], ['github_notebooks_3846_3862_p4.json', 98, 688]]\n"
     ]
    }
   ],
   "source": [
    "print(no_items)\n",
    "print(incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "0cf8cdf44046c"
   },
   "source": [
    "It looks like we had one file that was missing two notebooks for some reason. Not bad. \n",
    "\n",
    "We'll also want to check our coverage. Did we really cover all the byte sizes, or did we skip a range? Based on the results below, it seems we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "comet_cell_id": "c12cc868f8339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges = []\n",
    "gaps = []\n",
    "\n",
    "for f in nb_search_files:\n",
    "    parts = f.split('_')\n",
    "    if parts[4] == 'p1':\n",
    "        start = int(parts[2])\n",
    "        end = int(parts[3])\n",
    "        ranges.append([start, end])\n",
    "        \n",
    "ranges.sort(key=lambda x: x[0])\n",
    "\n",
    "for i, r in enumerate(ranges):\n",
    "    if i < len(ranges) - 2:\n",
    "        if ranges[i+1][0] != ranges[i][1] + 1:\n",
    "            gaps.append([ranges[i][1], ranges[i+1][0]])\n",
    "\n",
    "len(gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "6c8281136d122"
   },
   "source": [
    "## Create Dataframe & CSV\n",
    "\n",
    "Now we can create a dataframe to clean the data more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "comet_cell_id": "c2a1b8906b3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1261705, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_nbs)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "f596e71d81bd8"
   },
   "source": [
    "First off, it would be good to know if we have any duplicates. \n",
    "\n",
    "It turns out we have quite a few, (~12,000 the first time through) which are likely from either the files getting re-commited at a larger file-size while we were doing the search, or for errors in the query results. We will go ahead and drop all the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "comet_cell_id": "7f523acdb68a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_counts = df['html_url'].value_counts()\n",
    "len(nb_counts[nb_counts > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "comet_cell_id": "ea2623aee3f7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253620, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dedup = df.drop_duplicates(subset = df.columns[df.columns != 'query_page'], keep='first')\n",
    "df_dedup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "9caa7fbfdb0ce"
   },
   "outputs": [],
   "source": [
    "df_dedup.to_csv('../data/csv/nb_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "0914508ac1f89"
   },
   "source": [
    "So now I have 1.25 million files that I'm going to want to start [downloading](2_nb_download.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "ad77bb2291c37"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "comet_paths": [
   [
    "5e1a4093/Untitled.ipynb",
    1499986560627
   ],
   [
    "5e1a4093/data_profiling.ipynb",
    1500053226267
   ],
   [
    "b47baa7f/1_data_profiling.ipynb",
    1500065701105
   ],
   [
    "b47baa7f/1_nb_metadata_cleaning.ipynb",
    1501004151369
   ],
   [
    "d1dd24ab/1_nb_metadata_cleaning.ipynb",
    1501095229321
   ]
  ],
  "comet_tracking": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
