{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "72da9b0391bd6"
   },
   "source": [
    "# Searching for Jupyter Notebooks on Github\n",
    "\n",
    "This is the first notebook in a series designed to analyze over 1 million notebooks hosted publicly on GitHub.\n",
    "\n",
    "This particular notebook documents the first step of that process, searching notebooks on GitHub using GitHub's search API. See [Githubs API documentation](https://developer.github.com/v3/) and especially their [getting started guide](https://developer.github.com/v3/guides/getting-started/) for an introduction to using their search API.\n",
    "\n",
    "This search began at 10a on Tuesday, July 11, 2017 and ended at 3.40p on Thursday July 13, 2017. This search spanned multiple days due to GitHub's Search limit of 30 queries per minute and abuse rate-limiting which reduced effective search rate to around 10-15 queries per minute as searching more frequently than that often prompted GitHub to periodically demand we wait 60 seconds before completing our next search.\n",
    "\n",
    "In plain language, the query we used to search for notebooks asked for 1) files with \"ipynb\" in their path, 2) that ended with the \".ipynb\" extension and 3) were written in the language \"Jupyter Notebook\". Since this query returns about 1.25 million results, and GitHub will only give access to 1,000 results at a time, the overall query had to be split into over a thousand subqueries. We decided to split the query by looking for files of different ranges of sizes (e.g. 0-10 bytes, 10-20 bytes, and so on).\n",
    "\n",
    "More detailed information about what exact searches were performed when can be found in `logs/nb_metadata_query_log.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "ff5bcc9ca2cb5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "be01c5fe7c1e1"
   },
   "source": [
    "## Search with Recursion (0 - 6954 bytes)\n",
    "This first implementation of the search algorithm used recursion which python limits to ~1000 levels deep. Since I had to run many more search queries than 1,000, I opted for using a while loop instead (see below). I am keeping this code here for documentation's sake, but most of our API queries were run using the Recursion code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "3b4e104c4a025"
   },
   "outputs": [],
   "source": [
    "# http request authentication\n",
    "header = {'Authorization': 'token %s' % os.environ['GITHUB_TOKEN']}\n",
    "\n",
    "# initialize query request parameters\n",
    "base_url = 'https://api.github.com/search/code?l=Jupyter+Notebook&q=ipynb+in:path+extension:ipynb'\n",
    "page_url = '&per_page=100'\n",
    "\n",
    "\n",
    "def search_range_on_github(search_range):\n",
    "    # get search variables ready\n",
    "    start = search_range[0]\n",
    "    end = search_range[1]    \n",
    "    size_url = get_size_url(start, end)\n",
    "    search_url = base_url + size_url + page_url\n",
    "    \n",
    "    # query GitHub\n",
    "    r = requests.get(search_url, headers = header)\n",
    "    j = r.json()\n",
    "    h = r.headers\n",
    "    \n",
    "    # if asked to slow down due to abuse rate limiting, wait the alloted time\n",
    "    if h['Status'] == \"403 Forbidden\":\n",
    "        wait = h['Retry-After']\n",
    "        print(\"%s: Hit rate limit. Retry after %s seconds\" % (h['Date'], h['Retry-After']))\n",
    "        time.sleep(int(wait) + 1)\n",
    "        search_range_on_github(search_range)\n",
    "    \n",
    "    # get data about the query time and number of results\n",
    "    num_items = int(len(j['items']))\n",
    "    num_results = int(j['total_count'])\n",
    "    date = r.headers[\"Date\"]\n",
    "    \n",
    "    # log the query\n",
    "    log_string = \"%s: %s-%s bytes %s results\" % (date, start, end, num_results)\n",
    "    write_to_log(log_string)\n",
    "    \n",
    "    # continue to search until less than 1,000 results    \n",
    "    if num_results > 1000:\n",
    "        new_search_range = set_search_range(search_range, num_results)\n",
    "        schedule_search(new_search_range, h['X-RateLimit-Remaining'], h['X-RateLimit-Reset'])\n",
    "\n",
    "    # when less than 1,000 results \n",
    "    else:\n",
    "        # save the first page of data\n",
    "        save_result(j, start, end, 1)\n",
    "        log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, 1, num_items)\n",
    "        write_to_log(log_string)\n",
    "                  \n",
    "        # traverse pagination if needed    \n",
    "        if 'next' in r.links:\n",
    "            next_url = r.links['next']['url']\n",
    "            traverse_pagination(next_url, search_range, h['X-RateLimit-Remaining'], h['X-RateLimit-Reset'])\n",
    "        \n",
    "        # otherwise search the next range of notebook sizes       \n",
    "        else:\n",
    "            new_search_range = set_search_range(search_range, num_results)\n",
    "            schedule_search(new_search_range, h['X-RateLimit-Remaining'], h['X-RateLimit-Reset']) # h['X-RateLimit-Limit']\n",
    "\n",
    "def get_size_url(start, end):\n",
    "    # generate the url sring needed to search a range of file sizes\n",
    "    size_url = \"+size:\"\n",
    "    if end == start:\n",
    "        size_url += str(start)\n",
    "        return size_url\n",
    "    elif end > start:\n",
    "        size_url += str(start) + \"..\" + str(end)\n",
    "        return size_url\n",
    "    else:\n",
    "        print(\"Error: Search range end bigger than start %s - %s\" % ())\n",
    "            \n",
    "def schedule_search(search_range, remaing_queries, reset_time):\n",
    "    # delay the search to avoid being rate limited\n",
    "    if remaing_queries == 0:\n",
    "        time.sleep(reset_time - time.time() + 1)\n",
    "    else:\n",
    "        time.sleep(3)\n",
    "    search_range_on_github(search_range)\n",
    "    \n",
    "def save_result(json_result, start, end, page):\n",
    "    # save the result to a json file\n",
    "    filename = \"data/github_notebooks_%s_%s_p%s.json\" % (start, end, page)    \n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(json_result, json_file)\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = 'nb_metadata_query_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "# even if a query returns 1000 results, these are broken up to only have 100 per page\n",
    "# with a link between pages\n",
    "def traverse_pagination(url, search_range, remaing_queries, reset_time):\n",
    "    # wait to run query if needed\n",
    "    if remaing_queries == 0:\n",
    "        print(\"No queries left\")\n",
    "        time.sleep(reset_time - time.time() + 1)\n",
    "    else:\n",
    "        time.sleep(3)\n",
    "    \n",
    "    start = search_range[0]\n",
    "    end = search_range[1]\n",
    "    \n",
    "    # get this page's data\n",
    "    r = requests.get(url, headers = header)\n",
    "    j = r.json()\n",
    "    h = r.headers\n",
    "    \n",
    "    # if asked to slow down due to abuse rate limiting, wait the alloted time\n",
    "    if h['Status'] == \"403 Forbidden\":\n",
    "        wait = h['Retry-After']\n",
    "        print(\"%s: Hit rate limit. Retry after %s seconds\" % (h['Date'], h['Retry-After']))\n",
    "        time.sleep(int(wait) + 1)\n",
    "        traverse_pagination(url, search_range, remaing_queries, reset_time)\n",
    "    \n",
    "    # get data about the time and number of results returned by quer\n",
    "    num_results = int(j['total_count'])\n",
    "    num_items = len(j['items'])\n",
    "    page_num = url.split('&page=')[1]\n",
    "    date = r.headers[\"Date\"]\n",
    "    \n",
    "    # save the results and write to log file\n",
    "    save_result(j, start, end, page_num)\n",
    "    log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, page_num, num_items)\n",
    "    write_to_log(log_string)\n",
    "    \n",
    "    # keep iterating through links to next page of search results if multiple pages\n",
    "    if \"next\" in r.links:\n",
    "        next_url = r.links['next']['url']\n",
    "        traverse_pagination(next_url, search_range, h['X-RateLimit-Remaining'], h['X-RateLimit-Reset'])\n",
    "        \n",
    "    else:\n",
    "        new_search_range = set_search_range(search_range, num_results)\n",
    "        schedule_search(new_search_range, h['X-RateLimit-Remaining'], h['X-RateLimit-Reset'])\n",
    "    \n",
    "def set_search_range(prior_range, prior_num_results):\n",
    "    # look at prior search range\n",
    "    prior_start = prior_range[0]\n",
    "    prior_end = prior_range[1]\n",
    "    prior_delta = prior_end - prior_start\n",
    "    \n",
    "    # if too many results last time, reset the range to try and only get 1,000 results in the query\n",
    "    if prior_num_results > 1000:\n",
    "        # if the prior range was just one file size, we can't go any smaller, so just move on\n",
    "        if prior_delta == 0:\n",
    "            #log that there are more than 1,000 files of this size, and move on\n",
    "            log_string = \"TOO MANY RESULTS: %s-%s bytes, %s results\" % (prior_start, prior_end, prior_num_results)\n",
    "            write_to_log(log_string)\n",
    "            \n",
    "            new_delta = 10\n",
    "            start = prior_end + 1 \n",
    "            end = start + new_delta\n",
    "        else:\n",
    "            new_delta = int(prior_delta / 2)\n",
    "            start = prior_start\n",
    "            end = start + new_delta\n",
    "    # if under 1000 results, either increase the search size, or keep it the same size\n",
    "    else: \n",
    "        if prior_num_results < 500:\n",
    "            new_delta = int(prior_delta * 2)\n",
    "            start = prior_end + 1\n",
    "            end = start + new_delta\n",
    "        else:\n",
    "            new_delta = prior_delta\n",
    "            start = prior_end + 1\n",
    "            end = start + new_delta\n",
    "\n",
    "    # stop conditions\n",
    "    if int(start) > 100000000:\n",
    "        print(\"Start value too high. May have reached end of search\")\n",
    "        return\n",
    "    if int(end) > 100000000:\n",
    "        end = 100000000\n",
    "            \n",
    "    return [int(start), int(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "b301e65bdecea"
   },
   "outputs": [],
   "source": [
    "# this is the starting search range, we changed it over time when a bug caused the code\n",
    "# to crash and we needed to restart the process\n",
    "search_range = [4675,4695]            \n",
    "            \n",
    "# do first search\n",
    "search_range_on_github(search_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "ac7bef624a9d3"
   },
   "source": [
    "## Rewrite without recursion (6954 - 100,000,000 bytes)\n",
    "\n",
    "Turns out, python has a maximum depth for recursion as a safety check (somewhere around 998 calls). I hit that limit about every 8% of the data set or 80,000 notebooks. To try to avoid hitting that limit, I have rewritten the code to use while loops rather than recursion. \n",
    "\n",
    "I started using this search algorithm around the 6934-6954 byte range. You will notice several instances in the log of iterating over the 6955-6975 dataset due to a bug in the code, but after fixing that it looks like things ran smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "26cb549c3b542"
   },
   "outputs": [],
   "source": [
    "# http request authentication\n",
    "header = {'Authorization': 'token %s' % os.environ['GITHUB_TOKEN']}\n",
    "\n",
    "# initialize query request parameters\n",
    "base_url = 'https://api.github.com/search/code?l=Jupyter+Notebook&q=ipynb+in:path+extension:ipynb'\n",
    "page_url = '&per_page=100'\n",
    "\n",
    "\n",
    "def run_query_loop(search_range):\n",
    "    # set initial range\n",
    "    start = search_range[0]\n",
    "    end = search_range[1]\n",
    "    \n",
    "    # set inital loop management variables\n",
    "    remaing_queries = 30 \n",
    "    reset_time = time.time()\n",
    "    limited = False\n",
    "    wait_time = 0\n",
    "    \n",
    "    # so long as we have not reached the end (i.e. 100mb files)\n",
    "    while start < 100000000:\n",
    "        \n",
    "        # limit how often we query to prevent rate-limiting by GitHub\n",
    "        if limited:\n",
    "            time.sleep(wait_time)\n",
    "            limited = False\n",
    "        elif remaing_queries == 0:\n",
    "            time.sleep(reset_time - time.time() + 1)\n",
    "        else:\n",
    "            time.sleep(4)\n",
    "        \n",
    "        # compose search url\n",
    "        size_url = get_size_url(start, end)\n",
    "        search_url = base_url + size_url + page_url\n",
    "\n",
    "        # query GitHub\n",
    "        r = requests.get(search_url, headers = header)\n",
    "        j = r.json()\n",
    "        h = r.headers\n",
    "        \n",
    "        # handle abuse rate limiting\n",
    "        if h['Status'] == \"403 Forbidden\":\n",
    "            print(\"%s: Hit rate limit. Retry after %s seconds\" % (h['Date'], h['Retry-After']))\n",
    "            limited = True\n",
    "            wait_time = int(h['Retry-After'])\n",
    "            continue  \n",
    "\n",
    "        date = r.headers[\"Date\"]\n",
    "        num_items = int(len(j['items']))\n",
    "        num_results = int(j['total_count'])\n",
    "        remaing_queries = h['X-RateLimit-Remaining']\n",
    "        reset_time = int(h['X-RateLimit-Reset'])\n",
    "            \n",
    "        log_string = \"%s: %s-%s bytes %s results\" % (date, start, end, num_results)\n",
    "        write_to_log(log_string)\n",
    "\n",
    "        # continue to search until less than 1,000 results    \n",
    "        if num_results > 1000:\n",
    "            new_search_range = set_search_range([start, end], num_results)\n",
    "            start = new_search_range[0]\n",
    "            end = new_search_range[1]\n",
    "            continue\n",
    "\n",
    "        # when less than 1,000 results \n",
    "        else:\n",
    "            # save the first page of data\n",
    "            save_result(j, start, end, 1)\n",
    "            log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, 1, num_items)\n",
    "            write_to_log(log_string)\n",
    "\n",
    "            # traverse pagination if needed    \n",
    "            if 'next' in r.links:\n",
    "                next_url = r.links['next']['url']\n",
    "                another_page = True\n",
    "                \n",
    "                while another_page:\n",
    "                    if limited:\n",
    "                        time.sleep(wait_time)\n",
    "                        limited = False\n",
    "                    elif remaing_queries == 0:\n",
    "                        time.sleep(reset_time - time.time() + 1)\n",
    "                    else:\n",
    "                        time.sleep(4)\n",
    "\n",
    "                    rp = requests.get(next_url, headers = header)\n",
    "                    jp = rp.json()\n",
    "                    hp = rp.headers\n",
    "\n",
    "                    if hp['Status'] == \"403 Forbidden\":                        \n",
    "                        print(\"%s: Hit rate limit. Retry after %s seconds\" % (hp['Date'], hp['Retry-After']))\n",
    "                        limited = True\n",
    "                        wait_time = int(hp['Retry-After'])\n",
    "                        continue \n",
    "\n",
    "                    date = rp.headers[\"Date\"]\n",
    "                    num_results = int(jp['total_count'])\n",
    "                    num_items = len(jp['items'])\n",
    "                    page_num = next_url.split('&page=')[1]\n",
    "                    remaing_queries = hp['X-RateLimit-Remaining']\n",
    "                    reset_time = int(hp['X-RateLimit-Reset'])\n",
    "                    \n",
    "\n",
    "                    save_result(jp, start, end, page_num)\n",
    "                    log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, page_num, num_items)\n",
    "                    write_to_log(log_string)\n",
    "\n",
    "                    if \"next\" in rp.links:\n",
    "                        next_url = rp.links['next']['url']\n",
    "                    else:\n",
    "                        another_page = False\n",
    "           \n",
    "            # otherwise search the next range of notebook sizes       \n",
    "            new_search_range = set_search_range([start, end], num_results)\n",
    "            start = new_search_range[0]\n",
    "            end = new_search_range[1]\n",
    "            continue\n",
    "                \n",
    "    print(\"Loop has finished with range of %s-%s\" % (start, end))\n",
    "\n",
    "def get_size_url(start, end):\n",
    "    size_url = \"+size:\"\n",
    "    if end == start:\n",
    "        if start > 100000000:\n",
    "            size_url += \">\"\n",
    "            size_url += str(start)\n",
    "            return size_url\n",
    "        else:\n",
    "            size_url += str(start)\n",
    "            return size_url\n",
    "    elif end > start:\n",
    "        size_url += str(start) + \"..\" + str(end)\n",
    "        return size_url\n",
    "    else:\n",
    "        print(\"Error: Search range end bigger than start %s - %s\" % ())\n",
    "    \n",
    "def save_result(json_result, start, end, page):\n",
    "    filename = \"data/github_notebooks_%s_%s_p%s.json\" % (start, end, page)    \n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(json_result, json_file)\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = 'nb_metadata_query_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "    \n",
    "def set_search_range(prior_range, prior_num_results):\n",
    "    prior_start = prior_range[0]\n",
    "    prior_end = prior_range[1]\n",
    "    prior_delta = prior_end - prior_start\n",
    "    \n",
    "    if prior_num_results > 1000:\n",
    "        if prior_delta == 0:\n",
    "            #log that there are more than 1,000 files of this size, and move on\n",
    "            log_string = \"TOO MANY RESULTS: %s-%s bytes, %s results\" % (prior_start, prior_end, prior_num_results)\n",
    "            write_to_log(log_string)\n",
    "            \n",
    "            new_delta = 10\n",
    "            start = prior_end + 1 \n",
    "            end = start + new_delta\n",
    "        else:\n",
    "            new_delta = int(prior_delta / 2)\n",
    "            start = prior_start\n",
    "            end = start + new_delta\n",
    "    else: \n",
    "        if prior_num_results < 500:\n",
    "            new_delta = int(prior_delta * 2)\n",
    "            start = prior_end + 1\n",
    "            end = start + new_delta\n",
    "        else:\n",
    "            new_delta = prior_delta\n",
    "            start = prior_end + 1\n",
    "            end = start + new_delta\n",
    "\n",
    "    if int(start) > 100000000:\n",
    "        print(\"Start value too high. May have reached end of search\")\n",
    "        end = start\n",
    "            \n",
    "    return [int(start), int(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "comet_cell_id": "44bdf64443ae8"
   },
   "outputs": [],
   "source": [
    "search_range = [72088186,100000000]            \n",
    "            \n",
    "# do first search\n",
    "run_query_loop(search_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "f1d6bf55ddeed"
   },
   "source": [
    "\n",
    "And just to make sure, we'll do one final query of notebooks greater than 100mb in size to make sure we get them all. I believe Github limits file size to 100mb or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "comet_cell_id": "26a356228523d"
   },
   "outputs": [],
   "source": [
    "search_range = [100000000,1000000000]            \n",
    "            \n",
    "# do first search\n",
    "run_query_loop(search_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5fa4b1d4775b"
   },
   "source": [
    "## Additional Queries for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "1334fc5cdd204"
   },
   "source": [
    "While cleaning the data, we found we were missing some of the notebook metadata. Some query results were incomplete (i.e. GitHub did not return all the notebooks they said they would), or there were no items in the query response (possibly an error in our JSON writing code). \n",
    "\n",
    "Just in case it was an issue with the GitHub API, we searched for these ranges again. A better initial search alrorithm would check for these incomplete results at the time of the query and redo the search immediately. As it is, we may still miss some notebooks, or get duplicate notebooks if they have changed size in the time between the initial search and now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "c692108b356bd"
   },
   "outputs": [],
   "source": [
    "missing =  ['13924..13964',\n",
    "    '3111498..3193418',\n",
    "    '11956..11996',\n",
    "    '19659380..22280820',\n",
    "    '3423..3439',\n",
    "    '3846..3862',\n",
    "    '4590..4606',\n",
    "    '3998..4014',\n",
    "    '3183..3199',\n",
    "    '12530..12570',\n",
    "    '127088..127728',\n",
    "    '4234..4250',\n",
    "    '3812..3828',\n",
    "    '30355..30435',\n",
    "    '4336..4352',\n",
    "    '2877..2893',\n",
    "    '22780..22860',\n",
    "    '1337..1347',\n",
    "    '1626..1642',\n",
    "    '15523..15563',\n",
    "    '3863..3879',\n",
    "    '3913..3929',\n",
    "    '166617..167257',\n",
    "    '2435..2451',\n",
    "    '4455..4471',\n",
    "    '197385..198025',\n",
    "    '3964..3980',\n",
    "    '28330..28410',\n",
    "    '4151..4166', \n",
    "    '4184..4200',\n",
    "    '4201..4216',\n",
    "    '4506..4521',\n",
    "    '4521..4538',\n",
    "    '3745..3760',\n",
    "    '3761..3777',\n",
    "    '3880..3900',\n",
    "    '3901..3912',\n",
    "    '3661..3680',\n",
    "    '3681..3693',\n",
    "    '141831..142471',\n",
    "    '17368..17408',\n",
    "    '626..646',\n",
    "    '3576..3592',\n",
    "    '3778..3794',\n",
    "    '4100..4116',\n",
    "    '10931..10971',\n",
    "    '2418..2434',\n",
    "    '1059..1079',\n",
    "    '4184..4216',\n",
    "    '28978..29058',\n",
    "    '3745..3777',\n",
    "    '103996..104316',\n",
    "    '4506..4538',\n",
    "    '10396..10436',\n",
    "    '2792..2808',\n",
    "    '4066..4082',\n",
    "    '14867..14907',\n",
    "    '17204..17244',\n",
    "    '1864..1880',\n",
    "    '4083..4099',\n",
    "    '4167..4183',\n",
    "    '4117..4133',\n",
    "    '4370..4386',\n",
    "    '3728..3744',\n",
    "    '3694..3710',\n",
    "    '4217..4233',\n",
    "    '4438..4454',\n",
    "    '4015..4031',\n",
    "    '4387..4403',\n",
    "    '18879..18919',\n",
    "    '17368..17408',\n",
    "    '136703..137343',\n",
    "    '169181..169821',\n",
    "    '164694..165334',\n",
    "    '3829..3845',\n",
    "    '3440..3456',\n",
    "    '2005553..2046513',\n",
    "    '30922..31002',\n",
    "    '1231..1251',\n",
    "    '26791..26871',\n",
    "    '23023..23103',\n",
    "    '287066..288346',\n",
    "    '274256..275536',\n",
    "    '13637..13677',\n",
    "    '12120..12160',\n",
    "    '3880..3912',\n",
    "    '4641..4657',\n",
    "    '141831..142471',\n",
    "    '11423..11463',\n",
    "    '2619968..2660928',\n",
    "    '4489..4505',\n",
    "    '3406..3422',\n",
    "    '4032..4048',\n",
    "    '3661..3693',\n",
    "    '3559..3575',\n",
    "    '3644..3660',\n",
    "    '13596..13636',\n",
    "    '4134..4166']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "comet_cell_id": "b16a84e611e36"
   },
   "outputs": [],
   "source": [
    "# http request authentication\n",
    "header = {'Authorization': 'token %s' % os.environ['GITHUB_TOKEN']}\n",
    "\n",
    "# initialize query request parameters\n",
    "base_url = 'https://api.github.com/search/code?l=Jupyter+Notebook&q=ipynb+in:path+extension:ipynb'\n",
    "page_url = '&per_page=100'\n",
    "\n",
    "# set inital loop management variables\n",
    "remaing_queries = 30 \n",
    "reset_time = time.time()\n",
    "limited = False\n",
    "wait_time = 0\n",
    "\n",
    "def save_result(json_result, start, end, page):\n",
    "    filename = \"data/github_notebooks_%s_%s_p%s.json\" % (start, end, page)    \n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(json_result, json_file)\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = 'nb_metadata_query_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "for m in missing:\n",
    "    \n",
    "    not_started = True\n",
    "    \n",
    "    while not_started:\n",
    "    \n",
    "        start = int(m.split('..')[0])\n",
    "        end = int(m.split('..')[1])\n",
    "\n",
    "        # limit how often we query to prevent rate-limiting by GitHub\n",
    "        if limited:\n",
    "            time.sleep(wait_time)\n",
    "            limited = False\n",
    "        elif remaing_queries == 0:\n",
    "            time.sleep(reset_time - time.time() + 1)\n",
    "        else:\n",
    "            time.sleep(7)\n",
    "\n",
    "        # compose search url\n",
    "        size_url = '+size:' + m\n",
    "        search_url = base_url + size_url + page_url\n",
    "\n",
    "        # query GitHub\n",
    "        r = requests.get(search_url, headers = header)\n",
    "        j = r.json()\n",
    "        h = r.headers\n",
    "\n",
    "        # handle abuse rate limiting\n",
    "        if h['Status'] == \"403 Forbidden\":\n",
    "            print(\"%s: Hit rate limit. Retry after %s seconds\" % (h['Date'], h['Retry-After']))\n",
    "            limited = True\n",
    "            wait_time = int(h['Retry-After'])\n",
    "            continue\n",
    "        else:\n",
    "            not_started = False\n",
    "            print(m)\n",
    "\n",
    "        date = r.headers[\"Date\"]\n",
    "        num_items = int(len(j['items']))\n",
    "        num_results = int(j['total_count'])\n",
    "        remaing_queries = h['X-RateLimit-Remaining']\n",
    "        reset_time = int(h['X-RateLimit-Reset'])\n",
    "\n",
    "        log_string = \"%s: %s-%s bytes %s results\" % (date, start, end, num_results)\n",
    "        write_to_log(log_string)\n",
    "\n",
    "        # save the first page of data\n",
    "        save_result(j, start, end, 1)\n",
    "        log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, 1, num_items)\n",
    "        write_to_log(log_string)\n",
    "\n",
    "        # traverse pagination if needed    \n",
    "        if 'next' in r.links:\n",
    "            next_url = r.links['next']['url']\n",
    "            another_page = True\n",
    "\n",
    "            while another_page:\n",
    "                if limited:\n",
    "                    time.sleep(wait_time)\n",
    "                    limited = False\n",
    "                elif remaing_queries == 0:\n",
    "                    time.sleep(reset_time - time.time() + 1)\n",
    "                else:\n",
    "                    time.sleep(7)\n",
    "\n",
    "                rp = requests.get(next_url, headers = header)\n",
    "                jp = rp.json()\n",
    "                hp = rp.headers\n",
    "\n",
    "                if hp['Status'] == \"403 Forbidden\":                        \n",
    "                    print(\"%s: Hit rate limit. Retry after %s seconds\" % (hp['Date'], hp['Retry-After']))\n",
    "                    limited = True\n",
    "                    wait_time = int(hp['Retry-After'])\n",
    "                    continue \n",
    "\n",
    "                date = rp.headers[\"Date\"]\n",
    "                num_results = int(jp['total_count'])\n",
    "                num_items = len(jp['items'])\n",
    "                page_num = next_url.split('&page=')[1]\n",
    "                remaing_queries = hp['X-RateLimit-Remaining']\n",
    "                reset_time = int(hp['X-RateLimit-Reset'])\n",
    "\n",
    "\n",
    "                save_result(jp, start, end, page_num)\n",
    "                log_string = \"%s: %s-%s bytes p%s %s items\" % (date, start, end, page_num, num_items)\n",
    "                write_to_log(log_string)\n",
    "\n",
    "                if \"next\" in rp.links:\n",
    "                    next_url = rp.links['next']['url']\n",
    "                else:\n",
    "                    another_page = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "2cab69064c10a"
   },
   "source": [
    "# End\n",
    "\n",
    "And That's a wrap. We should now have metadata on 1.25 million jupyter notebooks on Github including their url, repository, owner, and so on. \n",
    "\n",
    "Now off to the [notebook metadata profiling](1_nb_metadata_cleaning.ipynb)  for an initial look at the data and basic  cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "48907cf5e4618"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "comet_paths": [
   [
    "5e1a4093/Untitled.ipynb",
    1499791964722
   ],
   [
    "5e1a4093/scraping_jupyter_notebooks_github.ipynb",
    1499792516726
   ],
   [
    "b47baa7f/0_github_api_search.ipynb",
    1500065289081
   ],
   [
    "b47baa7f/0_search_jupyter_nbs_github.ipynb",
    1501004088942
   ],
   [
    "d1dd24ab/0_search_jupyter_nbs_github.ipynb",
    1501095245008
   ],
   [
    "d1dd24ab/0_nb_metadata_download.ipynb",
    1501098395645
   ]
  ],
  "comet_tracking": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
